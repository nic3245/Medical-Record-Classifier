{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 19:29:06.093975: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "labels = [\"ABDOMINAL\",\n",
    "        \"ADVANCED-CAD\",\n",
    "        \"ALCOHOL-ABUSE\",\n",
    "        \"ASP-FOR-MI\",\n",
    "        \"CREATININE\",\n",
    "        \"DIETSUPP-2MOS\",\n",
    "        \"DRUG-ABUSE\",\n",
    "        \"ENGLISH\",\n",
    "        \"HBA1C\",\n",
    "        \"KETO-1YR\",\n",
    "        \"MAJOR-DIABETES\",\n",
    "        \"MAKES-DECISIONS\",\n",
    "        \"MI-6MOS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_note_data(labels, folder_name='train', separate=False):\n",
    "    if separate:\n",
    "        headers = [\"note1\", \"note2\", \"note3\", \"note4\", \"note5\"]\n",
    "        headers.extend(labels)\n",
    "        overall_df = pd.DataFrame(columns=headers)\n",
    "    else:\n",
    "        headers = [\"notes\"]\n",
    "        headers.extend(labels)\n",
    "        overall_df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    directory = os.path.join(current_directory, folder_name)\n",
    "    for filename in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            patient_num = os.path.splitext(filename)[0]\n",
    "            row_to_add = {}\n",
    "            # Load the XML file\n",
    "            tree = ET.parse(os.path.join(directory, filename))\n",
    "            root = tree.getroot()\n",
    "            # Access elements and attributes\n",
    "            for child in root:\n",
    "                if child.tag == \"TEXT\":\n",
    "                    if separate:\n",
    "                        notes = child.text.split(\"****************************************************************************************************\")\n",
    "                        notes = [note.strip() for note in notes if note.strip()]\n",
    "                        i = 1\n",
    "                        for note in notes:\n",
    "                            row_to_add[f\"note{i}\"] = note\n",
    "                            i += 1\n",
    "                        for j in range(i, 6):\n",
    "                            row_to_add[f\"note{j}\"] = \"\"\n",
    "                    else:\n",
    "                        note = child.text\n",
    "                        row_to_add['notes'] = note\n",
    "                if child.tag == \"TAGS\":\n",
    "                    for subchild in child:\n",
    "                        row_to_add[subchild.tag] = 1 if subchild.attrib.get('met') == 'met' else 0\n",
    "            overall_df.loc[patient_num] = row_to_add\n",
    "\n",
    "    return overall_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notes</th>\n",
       "      <th>ABDOMINAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>\\n\\nRecord date: 2106-02-12\\n\\nCampbell Orthop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>\\n\\nRecord date: 2079-05-12\\n\\n\\n\\n\\n\\nMERCY C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>\\n\\nRecord date: 2120-09-19\\n\\nPersonal Data a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>\\n\\nRecord date: 2067-11-24\\n\\n               ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>\\n\\nRecord date: 2094-02-16\\n\\nJENNIFER BOOKER...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 notes  ABDOMINAL\n",
       "100  \\n\\nRecord date: 2106-02-12\\n\\nCampbell Orthop...          0\n",
       "101  \\n\\nRecord date: 2079-05-12\\n\\n\\n\\n\\n\\nMERCY C...          0\n",
       "102  \\n\\nRecord date: 2120-09-19\\n\\nPersonal Data a...          1\n",
       "103  \\n\\nRecord date: 2067-11-24\\n\\n               ...          0\n",
       "104  \\n\\nRecord date: 2094-02-16\\n\\nJENNIFER BOOKER...          0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_notes_data = get_note_data(labels)\n",
    "patient_notes_data.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_for_label_combined_notes(df, label, num_epochs, save=False, save_name=None):\n",
    "    '''\n",
    "    Makes a fine-tuned ClinicalBERT model for a given label with data.\n",
    "\n",
    "    Arguments:\n",
    "    df - DataFrame containing a 'notes' column with the corresponding clinical notes\n",
    "    label - Str that is the name of the column to use as y/labels for the model training\n",
    "    save - Flag to save the model\n",
    "    save_name - What the directory for the model should be named\n",
    "\n",
    "    Returns:\n",
    "    TFAutoModelForSequenceClassification\n",
    "    '''\n",
    "    if save and save_name is None:\n",
    "        save_name = f'{label}_model'\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", from_pt=True)\n",
    "    clinical_notes = list(df['notes'])\n",
    "\n",
    "    # Tokenize and pad the clinical notes\n",
    "    tokenized_notes = tokenizer(clinical_notes, padding='max_length', max_length=512, truncation=True, return_tensors=\"tf\")\n",
    "    tokenized_data = dict(tokenized_notes)\n",
    "    labels = np.array(df[label])\n",
    "\n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3)\n",
    "    model.compile(optimizer=Adam(3e-5), metrics='accuracy')  # No loss argument!\n",
    "\n",
    "    model.fit(tokenized_data, labels, epochs=num_epochs, verbose=True, callbacks=[lr_scheduler])\n",
    "    if save:\n",
    "        tokenizer.save_pretrained(save_name)\n",
    "        model.save_pretrained(save_name)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_models_for_labels(labels, make_model_function, df, epochs, save=False):\n",
    "    '''\n",
    "    Makes a model for each label using the given function, data, and labels.\n",
    "\n",
    "    Arguments:\n",
    "    labels - List[Str] where each string is a label\n",
    "    make_model_function - Callable to make the model that takes in a df and label\n",
    "    df - DataFrame of the data\n",
    "    save - Flag to save each model\n",
    "\n",
    "    Returns:\n",
    "    Dict Str -> TFAutoModelForSequenceClassification\n",
    "    '''\n",
    "    models = {}\n",
    "    for label in labels:\n",
    "        print(f\"Making model for label {label}...\")\n",
    "        model = make_model_function(df, label, epochs, save=save)\n",
    "        print(\"Finished making model.\")\n",
    "        models[label] = model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of make_models_for_labels\n",
    "# # models = make_models_for_labels([\"ABDOMINAL\",\n",
    "#         \"ADVANCED-CAD\",\n",
    "#         \"ALCOHOL-ABUSE\",\n",
    "#         \"ASP-FOR-MI\",\n",
    "#         \"CREATININE\"], make_model_for_label_combined_notes, get_note_data(), epochs=2, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(note):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    pattern = r\"[\\n\\.]\"\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(pattern, note.lower())\n",
    "    # Split the text into tokens\n",
    "    tokens = [nltk.word_tokenize(sentence) for sentence in sentences if sentence.strip()]\n",
    "    # Remove stop words\n",
    "    #filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ABDOMINAL_model were not used when initializing TFBertForSequenceClassification: ['dropout_721']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ABDOMINAL_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABDOMINAL': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x17ed3b5d0>, BertTokenizerFast(name_or_path='ABDOMINAL_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "loaded_models = {}\n",
    "labels = [\"ABDOMINAL\"]\n",
    "for label in labels:\n",
    "    loaded_models[label] = (TFAutoModelForSequenceClassification.from_pretrained(f'{label}_model'), AutoTokenizer.from_pretrained(f'{label}_model'))\n",
    "print(loaded_models)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 168s 24s/step\n"
     ]
    }
   ],
   "source": [
    "threshold = .5\n",
    "def get_predictions(model, tokenizer, verbose=True):\n",
    "    tokenized_notes = tokenizer(list(patient_notes_data[\"notes\"]), padding='max_length', max_length=512, truncation=True, return_tensors=\"tf\")\n",
    "    tokenized_data = dict(tokenized_notes)\n",
    "    model_predictions = model.predict(tokenized_data)\n",
    "    logits = model_predictions.logits\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    list_probabilities = probabilities.numpy()\n",
    "    predictions = (list_probabilities[:,0] < .5).astype(int).tolist()\n",
    "    return predictions\n",
    "\n",
    "label_to_predictions = {}\n",
    "for label, model_tokenizer in loaded_models.items():\n",
    "    label_to_predictions[label] = get_predictions(model_tokenizer[0], model_tokenizer[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABDOMINAL Accuracy: 0.6188118811881188\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy_from_preds(predictions, true_labels, verbose=True):\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    if verbose:\n",
    "        print(f\"{label} Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "\n",
    "label_to_accuracy = {}\n",
    "for label, predictions in label_to_predictions.items():\n",
    "    true_labels = np.array(patient_notes_data[label])\n",
    "    accuracy = get_accuracy_from_preds(predictions, true_labels)\n",
    "    label_to_accuracy[label] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_to_accuracy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracies_for_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Northeastern/NLP/Final-Project/.conda/lib/python3.11/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/Desktop/Northeastern/NLP/Final-Project/.conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Northeastern/NLP/Final-Project/.conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/Desktop/Northeastern/NLP/Final-Project/.conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(label_to_accuracy).to_csv(\"accuracies_for_labels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
