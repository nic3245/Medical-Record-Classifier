{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlabu\\anaconda3\\envs\\tfenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "labels = [\"ABDOMINAL\",\n",
    "        \"ADVANCED-CAD\",\n",
    "        \"ALCOHOL-ABUSE\",\n",
    "        \"ASP-FOR-MI\",\n",
    "        \"CREATININE\",\n",
    "        \"DIETSUPP-2MOS\",\n",
    "        \"DRUG-ABUSE\",\n",
    "        \"ENGLISH\",\n",
    "        \"HBA1C\",\n",
    "        \"KETO-1YR\",\n",
    "        \"MAJOR-DIABETES\",\n",
    "        \"MAKES-DECISIONS\",\n",
    "        \"MI-6MOS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_note_data(labels, folder_name='train', separate=False):\n",
    "    if separate:\n",
    "        headers = [\"note1\", \"note2\", \"note3\", \"note4\", \"note5\"]\n",
    "        headers.extend(labels)\n",
    "        overall_df = pd.DataFrame(columns=headers)\n",
    "    else:\n",
    "        headers = [\"notes\"]\n",
    "        headers.extend(labels)\n",
    "        overall_df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    directory = os.path.join(current_directory, folder_name)\n",
    "    for filename in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            patient_num = os.path.splitext(filename)[0]\n",
    "            row_to_add = {}\n",
    "            # Load the XML file\n",
    "            tree = ET.parse(os.path.join(directory, filename))\n",
    "            root = tree.getroot()\n",
    "            # Access elements and attributes\n",
    "            for child in root:\n",
    "                if child.tag == \"TEXT\":\n",
    "                    if separate:\n",
    "                        notes = child.text.split(\"****************************************************************************************************\")\n",
    "                        notes = [note.strip() for note in notes if note.strip()]\n",
    "                        i = 1\n",
    "                        for note in notes:\n",
    "                            row_to_add[f\"note{i}\"] = note\n",
    "                            i += 1\n",
    "                        for j in range(i, 6):\n",
    "                            row_to_add[f\"note{j}\"] = \"\"\n",
    "                    else:\n",
    "                        note = child.text\n",
    "                        row_to_add['notes'] = note\n",
    "                if child.tag == \"TAGS\":\n",
    "                    for subchild in child:\n",
    "                        row_to_add[subchild.tag] = 1 if subchild.attrib.get('met') == 'met' else 0\n",
    "            overall_df.loc[patient_num] = row_to_add\n",
    "\n",
    "    return overall_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notes</th>\n",
       "      <th>ABDOMINAL</th>\n",
       "      <th>ADVANCED-CAD</th>\n",
       "      <th>ALCOHOL-ABUSE</th>\n",
       "      <th>ASP-FOR-MI</th>\n",
       "      <th>CREATININE</th>\n",
       "      <th>DIETSUPP-2MOS</th>\n",
       "      <th>DRUG-ABUSE</th>\n",
       "      <th>ENGLISH</th>\n",
       "      <th>HBA1C</th>\n",
       "      <th>KETO-1YR</th>\n",
       "      <th>MAJOR-DIABETES</th>\n",
       "      <th>MAKES-DECISIONS</th>\n",
       "      <th>MI-6MOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>\\n\\nRecord date: 2106-02-12\\n\\nCampbell Orthop...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>\\n\\nRecord date: 2079-05-12\\n\\n\\n\\n\\n\\nMERCY C...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>\\n\\nRecord date: 2120-09-19\\n\\nPersonal Data a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>\\n\\nRecord date: 2067-11-24\\n\\n               ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>\\n\\nRecord date: 2094-02-16\\n\\nJENNIFER BOOKER...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 notes  ABDOMINAL  \\\n",
       "100  \\n\\nRecord date: 2106-02-12\\n\\nCampbell Orthop...          0   \n",
       "101  \\n\\nRecord date: 2079-05-12\\n\\n\\n\\n\\n\\nMERCY C...          0   \n",
       "102  \\n\\nRecord date: 2120-09-19\\n\\nPersonal Data a...          1   \n",
       "103  \\n\\nRecord date: 2067-11-24\\n\\n               ...          0   \n",
       "104  \\n\\nRecord date: 2094-02-16\\n\\nJENNIFER BOOKER...          0   \n",
       "\n",
       "     ADVANCED-CAD  ALCOHOL-ABUSE  ASP-FOR-MI  CREATININE  DIETSUPP-2MOS  \\\n",
       "100             1              0           1           0              1   \n",
       "101             1              0           1           0              0   \n",
       "102             1              0           1           0              0   \n",
       "103             1              0           1           0              0   \n",
       "104             1              0           0           0              0   \n",
       "\n",
       "     DRUG-ABUSE  ENGLISH  HBA1C  KETO-1YR  MAJOR-DIABETES  MAKES-DECISIONS  \\\n",
       "100           0        1      0         0               1                1   \n",
       "101           0        1      0         0               0                1   \n",
       "102           0        1      0         0               1                1   \n",
       "103           0        1      0         0               0                1   \n",
       "104           0        1      1         0               1                1   \n",
       "\n",
       "     MI-6MOS  \n",
       "100        1  \n",
       "101        0  \n",
       "102        0  \n",
       "103        0  \n",
       "104        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_notes_data = get_note_data(labels, folder_name='test')\n",
    "patient_notes_data.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_for_label_combined_notes(df, label, num_epochs, save=False, save_name=None):\n",
    "    '''\n",
    "    Makes a fine-tuned ClinicalBERT model for a given label with data.\n",
    "\n",
    "    Arguments:\n",
    "    df - DataFrame containing a 'notes' column with the corresponding clinical notes\n",
    "    label - Str that is the name of the column to use as y/labels for the model training\n",
    "    save - Flag to save the model\n",
    "    save_name - What the directory for the model should be named\n",
    "\n",
    "    Returns:\n",
    "    TFAutoModelForSequenceClassification\n",
    "    '''\n",
    "    if save and save_name is None:\n",
    "        save_name = f'{label}_model'\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", from_pt=True)\n",
    "    clinical_notes = list(df['notes'])\n",
    "\n",
    "    # Tokenize and pad the clinical notes\n",
    "    tokenized_notes = tokenizer(clinical_notes, padding='max_length', max_length=512, truncation=True, return_tensors=\"tf\")\n",
    "    tokenized_data = dict(tokenized_notes)\n",
    "    labels = np.array(df[label])\n",
    "\n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, monitor='loss')\n",
    "    model.compile(optimizer=Adam(3e-5), metrics='accuracy')  # No loss argument!\n",
    "\n",
    "    model.fit(tokenized_data, labels, epochs=num_epochs, verbose=True, callbacks=[lr_scheduler])\n",
    "    if save:\n",
    "        tokenizer.save_pretrained(save_name)\n",
    "        model.save_pretrained(save_name)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_models_for_labels(labels, make_model_function, df, epochs, save=False):\n",
    "    '''\n",
    "    Makes a model for each label using the given function, data, and labels.\n",
    "\n",
    "    Arguments:\n",
    "    labels - List[Str] where each string is a label\n",
    "    make_model_function - Callable to make the model that takes in a df and label\n",
    "    df - DataFrame of the data\n",
    "    save - Flag to save each model\n",
    "\n",
    "    Returns:\n",
    "    Dict Str -> TFAutoModelForSequenceClassification\n",
    "    '''\n",
    "    models = {}\n",
    "    for label in labels:\n",
    "        print(f\"Making model for label {label}...\")\n",
    "        model = make_model_function(df, label, epochs, save=save)\n",
    "        print(\"Finished making model.\")\n",
    "        models[label] = model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making model for label ABDOMINAL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 11:11:49.721182: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 151s 20s/step - loss: 0.7508 - accuracy: 0.5000 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.6741 - accuracy: 0.6386 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 137s 19s/step - loss: 0.6553 - accuracy: 0.6238 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 139s 20s/step - loss: 0.6598 - accuracy: 0.6782 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.6241 - accuracy: 0.6238 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label ADVANCED-CAD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 152s 20s/step - loss: 0.6828 - accuracy: 0.5644 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 143s 20s/step - loss: 0.6526 - accuracy: 0.6139 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.6589 - accuracy: 0.6287 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.6063 - accuracy: 0.6931 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.5643 - accuracy: 0.7277 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label ALCOHOL-ABUSE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 150s 20s/step - loss: 0.3919 - accuracy: 0.9010 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 143s 20s/step - loss: 0.1591 - accuracy: 0.9653 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.1487 - accuracy: 0.9653 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.1451 - accuracy: 0.9653 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.1222 - accuracy: 0.9653 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label ASP-FOR-MI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 152s 20s/step - loss: 0.5432 - accuracy: 0.7822 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 144s 20s/step - loss: 0.5099 - accuracy: 0.8020 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 144s 20s/step - loss: 0.4885 - accuracy: 0.8020 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 143s 20s/step - loss: 0.4978 - accuracy: 0.8020 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 143s 20s/step - loss: 0.4685 - accuracy: 0.8020 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label CREATININE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 152s 20s/step - loss: 0.7002 - accuracy: 0.5347 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 145s 20s/step - loss: 0.6880 - accuracy: 0.5941 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 453s 72s/step - loss: 0.6670 - accuracy: 0.6485 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 257s 37s/step - loss: 0.6615 - accuracy: 0.5891 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.6566 - accuracy: 0.6089 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label DIETSUPP-2MOS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 147s 19s/step - loss: 0.7015 - accuracy: 0.5050 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 141s 20s/step - loss: 0.6854 - accuracy: 0.5099 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 145s 20s/step - loss: 0.6780 - accuracy: 0.5446 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 145s 20s/step - loss: 0.6591 - accuracy: 0.6089 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 141s 20s/step - loss: 0.6106 - accuracy: 0.6881 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label DRUG-ABUSE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 145s 19s/step - loss: 0.2672 - accuracy: 0.9406 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 137s 19s/step - loss: 0.2301 - accuracy: 0.9406 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 136s 19s/step - loss: 0.2303 - accuracy: 0.9406 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 138s 19s/step - loss: 0.2254 - accuracy: 0.9406 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 140s 20s/step - loss: 0.2205 - accuracy: 0.9406 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label ENGLISH...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 150s 20s/step - loss: 0.4379 - accuracy: 0.8218 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 137s 19s/step - loss: 0.2119 - accuracy: 0.9505 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 141s 20s/step - loss: 0.2089 - accuracy: 0.9505 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 136s 19s/step - loss: 0.1914 - accuracy: 0.9505 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 136s 19s/step - loss: 0.1952 - accuracy: 0.9505 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label HBA1C...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 146s 19s/step - loss: 0.6478 - accuracy: 0.6584 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 137s 19s/step - loss: 0.6099 - accuracy: 0.6733 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 135s 19s/step - loss: 0.5864 - accuracy: 0.6881 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 134s 19s/step - loss: 0.5685 - accuracy: 0.7129 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 136s 19s/step - loss: 0.4737 - accuracy: 0.8119 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label KETO-1YR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 149s 20s/step - loss: 0.4241 - accuracy: 0.8317 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.0866 - accuracy: 0.9950 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 143s 20s/step - loss: 0.0444 - accuracy: 0.9950 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.0349 - accuracy: 0.9950 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 142s 20s/step - loss: 0.0308 - accuracy: 0.9950 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label MAJOR-DIABETES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 166s 21s/step - loss: 0.7116 - accuracy: 0.5248 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 143s 20s/step - loss: 0.6842 - accuracy: 0.5347 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 150s 21s/step - loss: 0.6901 - accuracy: 0.5495 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 143s 19s/step - loss: 0.6574 - accuracy: 0.6040 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 141s 20s/step - loss: 0.6276 - accuracy: 0.6584 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label MAKES-DECISIONS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 151s 19s/step - loss: 0.3239 - accuracy: 0.9604 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 136s 19s/step - loss: 0.1673 - accuracy: 0.9604 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 146s 20s/step - loss: 0.1745 - accuracy: 0.9604 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 143s 20s/step - loss: 0.1709 - accuracy: 0.9604 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 143s 20s/step - loss: 0.1685 - accuracy: 0.9604 - lr: 3.0000e-05\n",
      "Finished making model.\n",
      "Making model for label MI-6MOS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 159s 20s/step - loss: 0.5295 - accuracy: 0.7624 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 148s 21s/step - loss: 0.3043 - accuracy: 0.9109 - lr: 3.0000e-05\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 143s 20s/step - loss: 0.3000 - accuracy: 0.9109 - lr: 3.0000e-05\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 141s 20s/step - loss: 0.2980 - accuracy: 0.9109 - lr: 3.0000e-05\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 147s 21s/step - loss: 0.2962 - accuracy: 0.9109 - lr: 3.0000e-05\n",
      "Finished making model.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of make_models_for_labels\n",
    "# models = make_models_for_labels(labels, make_model_for_label_combined_notes, get_note_data(labels), epochs=5, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(note):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    pattern = r\"[\\n\\.]\"\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(pattern, note.lower())\n",
    "    # Split the text into tokens\n",
    "    tokens = [nltk.word_tokenize(sentence) for sentence in sentences if sentence.strip()]\n",
    "    # Remove stop words\n",
    "    #filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ABDOMINAL_model were not used when initializing TFBertForSequenceClassification: ['dropout_721']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ABDOMINAL_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABDOMINAL': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x17ed3b5d0>, BertTokenizerFast(name_or_path='ABDOMINAL_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "loaded_models = {}\n",
    "labels = [\"ABDOMINAL\"]\n",
    "for label in labels:\n",
    "    loaded_models[label] = (TFAutoModelForSequenceClassification.from_pretrained(f'{label}_model'), AutoTokenizer.from_pretrained(f'{label}_model'))\n",
    "print(loaded_models)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 168s 24s/step\n"
     ]
    }
   ],
   "source": [
    "threshold = .5\n",
    "def get_predictions(model, tokenizer, verbose=True):\n",
    "    tokenized_notes = tokenizer(list(patient_notes_data[\"notes\"]), padding='max_length', max_length=512, truncation=True, return_tensors=\"tf\")\n",
    "    tokenized_data = dict(tokenized_notes)\n",
    "    model_predictions = model.predict(tokenized_data)\n",
    "    logits = model_predictions.logits\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    list_probabilities = probabilities.numpy()\n",
    "    predictions = (list_probabilities[:,0] < .5).astype(int).tolist()\n",
    "    return predictions\n",
    "\n",
    "label_to_predictions = {}\n",
    "for label, model_tokenizer in loaded_models.items():\n",
    "    label_to_predictions[label] = get_predictions(model_tokenizer[0], model_tokenizer[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABDOMINAL Accuracy: 0.6188118811881188\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy_from_preds(predictions, true_labels, verbose=True):\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    if verbose:\n",
    "        print(f\"{label} Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "\n",
    "label_to_accuracy = {}\n",
    "for label, predictions in label_to_predictions.items():\n",
    "    true_labels = np.array(patient_notes_data[label])\n",
    "    accuracy = get_accuracy_from_preds(predictions, true_labels)\n",
    "    label_to_accuracy[label] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_to_accuracy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracies_for_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Northeastern/NLP/Final-Project/.conda/lib/python3.11/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/Desktop/Northeastern/NLP/Final-Project/.conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Northeastern/NLP/Final-Project/.conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/Desktop/Northeastern/NLP/Final-Project/.conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(label_to_accuracy).to_csv(\"accuracies_for_labels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
