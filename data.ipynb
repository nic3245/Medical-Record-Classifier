{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlabu\\anaconda3\\envs\\tfenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "labels = [\"ABDOMINAL\",\n",
    "        \"ADVANCED-CAD\",\n",
    "        \"ALCOHOL-ABUSE\",\n",
    "        \"ASP-FOR-MI\",\n",
    "        \"CREATININE\",\n",
    "        \"DIETSUPP-2MOS\",\n",
    "        \"DRUG-ABUSE\",\n",
    "        \"ENGLISH\",\n",
    "        \"HBA1C\",\n",
    "        \"KETO-1YR\",\n",
    "        \"MAJOR-DIABETES\",\n",
    "        \"MAKES-DECISIONS\",\n",
    "        \"MI-6MOS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_note_data(labels, folder_name='train', separate=False):\n",
    "    if separate:\n",
    "        headers = [\"note1\", \"note2\", \"note3\", \"note4\", \"note5\"]\n",
    "        headers.extend(labels)\n",
    "        overall_df = pd.DataFrame(columns=headers)\n",
    "    else:\n",
    "        headers = [\"notes\"]\n",
    "        headers.extend(labels)\n",
    "        overall_df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    directory = os.path.join(current_directory, folder_name)\n",
    "    for filename in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            patient_num = os.path.splitext(filename)[0]\n",
    "            row_to_add = {}\n",
    "            # Load the XML file\n",
    "            tree = ET.parse(os.path.join(directory, filename))\n",
    "            root = tree.getroot()\n",
    "            # Access elements and attributes\n",
    "            for child in root:\n",
    "                if child.tag == \"TEXT\":\n",
    "                    if separate:\n",
    "                        notes = child.text.split(\"****************************************************************************************************\")\n",
    "                        notes = [note.strip() for note in notes if note.strip()]\n",
    "                        i = 1\n",
    "                        for note in notes:\n",
    "                            row_to_add[f\"note{i}\"] = note\n",
    "                            i += 1\n",
    "                        for j in range(i, 6):\n",
    "                            row_to_add[f\"note{j}\"] = \"\"\n",
    "                    else:\n",
    "                        note = child.text\n",
    "                        row_to_add['notes'] = note\n",
    "                if child.tag == \"TAGS\":\n",
    "                    for subchild in child:\n",
    "                        row_to_add[subchild.tag] = 1 if subchild.attrib.get('met') == 'met' else 0\n",
    "            overall_df.loc[patient_num] = row_to_add\n",
    "\n",
    "    return overall_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notes</th>\n",
       "      <th>ABDOMINAL</th>\n",
       "      <th>ADVANCED-CAD</th>\n",
       "      <th>ALCOHOL-ABUSE</th>\n",
       "      <th>ASP-FOR-MI</th>\n",
       "      <th>CREATININE</th>\n",
       "      <th>DIETSUPP-2MOS</th>\n",
       "      <th>DRUG-ABUSE</th>\n",
       "      <th>ENGLISH</th>\n",
       "      <th>HBA1C</th>\n",
       "      <th>KETO-1YR</th>\n",
       "      <th>MAJOR-DIABETES</th>\n",
       "      <th>MAKES-DECISIONS</th>\n",
       "      <th>MI-6MOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>\\n\\nRecord date: 2106-02-12\\n\\nCampbell Orthop...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>\\n\\nRecord date: 2079-05-12\\n\\n\\n\\n\\n\\nMERCY C...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>\\n\\nRecord date: 2120-09-19\\n\\nPersonal Data a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>\\n\\nRecord date: 2067-11-24\\n\\n               ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>\\n\\nRecord date: 2094-02-16\\n\\nJENNIFER BOOKER...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 notes  ABDOMINAL  \\\n",
       "100  \\n\\nRecord date: 2106-02-12\\n\\nCampbell Orthop...          0   \n",
       "101  \\n\\nRecord date: 2079-05-12\\n\\n\\n\\n\\n\\nMERCY C...          0   \n",
       "102  \\n\\nRecord date: 2120-09-19\\n\\nPersonal Data a...          1   \n",
       "103  \\n\\nRecord date: 2067-11-24\\n\\n               ...          0   \n",
       "104  \\n\\nRecord date: 2094-02-16\\n\\nJENNIFER BOOKER...          0   \n",
       "\n",
       "     ADVANCED-CAD  ALCOHOL-ABUSE  ASP-FOR-MI  CREATININE  DIETSUPP-2MOS  \\\n",
       "100             1              0           1           0              1   \n",
       "101             1              0           1           0              0   \n",
       "102             1              0           1           0              0   \n",
       "103             1              0           1           0              0   \n",
       "104             1              0           0           0              0   \n",
       "\n",
       "     DRUG-ABUSE  ENGLISH  HBA1C  KETO-1YR  MAJOR-DIABETES  MAKES-DECISIONS  \\\n",
       "100           0        1      0         0               1                1   \n",
       "101           0        1      0         0               0                1   \n",
       "102           0        1      0         0               1                1   \n",
       "103           0        1      0         0               0                1   \n",
       "104           0        1      1         0               1                1   \n",
       "\n",
       "     MI-6MOS  \n",
       "100        1  \n",
       "101        0  \n",
       "102        0  \n",
       "103        0  \n",
       "104        1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_notes_data = get_note_data(labels)\n",
    "patient_notes_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_for_label_combined_notes(df, label, num_epochs, save=False, save_name=None):\n",
    "    '''\n",
    "    Makes a fine-tuned ClinicalBERT model for a given label with data.\n",
    "\n",
    "    Arguments:\n",
    "    df - DataFrame containing a 'notes' column with the corresponding clinical notes\n",
    "    label - Str that is the name of the column to use as y/labels for the model training\n",
    "    save - Flag to save the model\n",
    "    save_name - What the directory for the model should be named\n",
    "\n",
    "    Returns:\n",
    "    TFAutoModelForSequenceClassification\n",
    "    '''\n",
    "    if save and save_name is None:\n",
    "        save_name = f'{label}_model'\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", from_pt=True)\n",
    "    clinical_notes = list(df['notes'])\n",
    "\n",
    "    # Tokenize and pad the clinical notes\n",
    "    tokenized_notes = tokenizer(clinical_notes, padding='max_length', max_length=512, truncation=True, return_tensors=\"tf\")\n",
    "    tokenized_data = dict(tokenized_notes)\n",
    "    labels = np.array(df[label])\n",
    "\n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3)\n",
    "    model.compile(optimizer=Adam(3e-5), metrics='accuracy')  # No loss argument!\n",
    "\n",
    "    model.fit(tokenized_data, labels, epochs=num_epochs, verbose=True, callbacks=[lr_scheduler])\n",
    "    if save:\n",
    "        tokenizer.save_pretrained(save_name)\n",
    "        model.save_pretrained(save_name)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_models_for_labels(labels, make_model_function, df, epochs, save=False):\n",
    "    '''\n",
    "    Makes a model for each label using the given function, data, and labels.\n",
    "\n",
    "    Arguments:\n",
    "    labels - List[Str] where each string is a label\n",
    "    make_model_function - Callable to make the model that takes in a df and label\n",
    "    df - DataFrame of the data\n",
    "    save - Flag to save each model\n",
    "\n",
    "    Returns:\n",
    "    Dict Str -> TFAutoModelForSequenceClassification\n",
    "    '''\n",
    "    models = {}\n",
    "    for label in labels:\n",
    "        print(f\"Making model for label {label}...\")\n",
    "        model = make_model_function(df, label, epochs, save=save)\n",
    "        print(\"Finished making model.\")\n",
    "        models[label] = model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of make_models_for_labels\n",
    "# # models = make_models_for_labels([\"ABDOMINAL\",\n",
    "#         \"ADVANCED-CAD\",\n",
    "#         \"ALCOHOL-ABUSE\",\n",
    "#         \"ASP-FOR-MI\",\n",
    "#         \"CREATININE\"], make_model_for_label_combined_notes, get_note_data(), epochs=2, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(note):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    pattern = r\"[\\n\\.]\"\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(pattern, note.lower())\n",
    "    # Split the text into tokens\n",
    "    tokens = [nltk.word_tokenize(sentence) for sentence in sentences if sentence.strip()]\n",
    "    # Remove stop words\n",
    "    #filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlabu\\anaconda3\\envs\\tfenv\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some layers from the model checkpoint at ABDOMINAL_model were not used when initializing TFBertForSequenceClassification: ['dropout_721']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ABDOMINAL_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at ADVANCED-CAD_model were not used when initializing TFBertForSequenceClassification: ['dropout_759']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ADVANCED-CAD_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at ALCOHOL-ABUSE_model were not used when initializing TFBertForSequenceClassification: ['dropout_797']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ALCOHOL-ABUSE_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at ASP-FOR-MI_model were not used when initializing TFBertForSequenceClassification: ['dropout_835']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ASP-FOR-MI_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at CREATININE_model were not used when initializing TFBertForSequenceClassification: ['dropout_873']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at CREATININE_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at DIETSUPP-2MOS_model were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at DIETSUPP-2MOS_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at DRUG-ABUSE_model were not used when initializing TFBertForSequenceClassification: ['dropout_113']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at DRUG-ABUSE_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at ENGLISH_model were not used when initializing TFBertForSequenceClassification: ['dropout_151']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ENGLISH_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at HBA1C_model were not used when initializing TFBertForSequenceClassification: ['dropout_189']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at HBA1C_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at KETO-1YR_model were not used when initializing TFBertForSequenceClassification: ['dropout_227']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at KETO-1YR_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at MAJOR-DIABETES_model were not used when initializing TFBertForSequenceClassification: ['dropout_265']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at MAJOR-DIABETES_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at MAKES-DECISIONS_model were not used when initializing TFBertForSequenceClassification: ['dropout_303']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at MAKES-DECISIONS_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at MI-6MOS_model were not used when initializing TFBertForSequenceClassification: ['dropout_341']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at MI-6MOS_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABDOMINAL': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x00000282F94CF1C0>, BertTokenizerFast(name_or_path='ABDOMINAL_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'ADVANCED-CAD': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x00000282FBB8F670>, BertTokenizerFast(name_or_path='ADVANCED-CAD_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'ALCOHOL-ABUSE': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x00000282A008E500>, BertTokenizerFast(name_or_path='ALCOHOL-ABUSE_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'ASP-FOR-MI': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x00000282A029A440>, BertTokenizerFast(name_or_path='ASP-FOR-MI_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'CREATININE': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x00000282A049ADD0>, BertTokenizerFast(name_or_path='CREATININE_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'DIETSUPP-2MOS': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x00000282D6FA3580>, BertTokenizerFast(name_or_path='DIETSUPP-2MOS_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'DRUG-ABUSE': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x000002830B421F60>, BertTokenizerFast(name_or_path='DRUG-ABUSE_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'ENGLISH': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x000002830B577820>, BertTokenizerFast(name_or_path='ENGLISH_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'HBA1C': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x0000028321767CD0>, BertTokenizerFast(name_or_path='HBA1C_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'KETO-1YR': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x000002834BBFF280>, BertTokenizerFast(name_or_path='KETO-1YR_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'MAJOR-DIABETES': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x00000283A7F8A050>, BertTokenizerFast(name_or_path='MAJOR-DIABETES_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'MAKES-DECISIONS': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x00000283B7E01510>, BertTokenizerFast(name_or_path='MAKES-DECISIONS_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)), 'MI-6MOS': (<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x00000283CE05FDF0>, BertTokenizerFast(name_or_path='MI-6MOS_model', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True))}\n"
     ]
    }
   ],
   "source": [
    "loaded_models = {}\n",
    "for label in labels:\n",
    "    loaded_models[label] = (TFAutoModelForSequenceClassification.from_pretrained(f'{label}_model'), AutoTokenizer.from_pretrained(f'{label}_model'))\n",
    "print(loaded_models)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 108s 16s/step\n",
      "3/7 [===========>..................] - ETA: 2:29"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, tokenizer, verbose=True):\n",
    "    tokenized_notes = tokenizer(list(patient_notes_data[\"notes\"]), padding='max_length', max_length=512, truncation=True, return_tensors=\"tf\")\n",
    "    tokenized_data = dict(tokenized_notes)\n",
    "    predictions = model.predict(tokenized_data)\n",
    "    return predictions\n",
    "\n",
    "label_to_predictions = {}\n",
    "for label, model_tokenizer in loaded_models.items():\n",
    "    label_to_predictions[label] = get_predictions(model_tokenizer[0], model_tokenizer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2042538017.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def get_accuracy_from_preds():\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy_from_preds(predictions, true_labels, verbose=True):\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    if verbose:\n",
    "        print(f\"{label} Accuracy: {accuracy}\")\n",
    "\n",
    "label_to_accuracy = {}\n",
    "for label, predictions in label_to_predictions.items():\n",
    "    true_labels = np.array(patient_notes_data[label])\n",
    "    get_accuracy_from_preds(predictions, true_labels)\n",
    "print(label_to_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(label_to_accuracy).to_csv(\"accuracies_for_labels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
