{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlabu\\anaconda3\\envs\\tfenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "labels = [\"ABDOMINAL\",\n",
    "        \"ADVANCED-CAD\",\n",
    "        \"ALCOHOL-ABUSE\",\n",
    "        \"ASP-FOR-MI\",\n",
    "        \"CREATININE\",\n",
    "        \"DIETSUPP-2MOS\",\n",
    "        \"DRUG-ABUSE\",\n",
    "        \"ENGLISH\",\n",
    "        \"HBA1C\",\n",
    "        \"KETO-1YR\",\n",
    "        \"MAJOR-DIABETES\",\n",
    "        \"MAKES-DECISIONS\",\n",
    "        \"MI-6MOS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_note_data(folder_name='train', separate=False):\n",
    "    if separate:\n",
    "        overall_df = pd.DataFrame(columns=[\"note1\", \"note2\", \"note3\", \"note4\", \"note5\"].extend(labels))\n",
    "    else:\n",
    "        overall_df = pd.DataFrame(columns=[\"notes\"].extend(labels))\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    directory = os.path.join(current_directory, folder_name)\n",
    "    for filename in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            patient_num = os.path.splitext(filename)[0]\n",
    "            row_to_add = {}\n",
    "            # Load the XML file\n",
    "            tree = ET.parse(os.path.join(directory, filename))\n",
    "            root = tree.getroot()\n",
    "            # Access elements and attributes\n",
    "            for child in root:\n",
    "                if child.tag == \"TEXT\":\n",
    "                    if separate:\n",
    "                        notes = child.text.split(\"****************************************************************************************************\")\n",
    "                        notes = [note.strip() for note in notes if note.strip()]\n",
    "                        i = 1\n",
    "                        for note in notes:\n",
    "                            row_to_add[f\"note{i}\"] = note\n",
    "                            i += 1\n",
    "                        for j in range(i, 6):\n",
    "                            row_to_add[f\"note{j}\"] = \"\"\n",
    "                    else:\n",
    "                        note = child.text\n",
    "                        row_to_add['notes'] = note\n",
    "                if child.tag == \"TAGS\":\n",
    "                    for subchild in child:\n",
    "                        row_to_add[subchild.tag] = 1 if subchild.attrib.get('met') == 'met' else 0\n",
    "            overall_df.loc[patient_num] = row_to_add\n",
    "\n",
    "    return overall_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\nlabu\\\\Downloads\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m patient_notes_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_note_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m patient_notes_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mget_note_data\u001b[1;34m(folder_name, separate)\u001b[0m\n\u001b[0;32m      7\u001b[0m current_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m      8\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_directory, folder_name)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename)):\n\u001b[0;32m     11\u001b[0m         patient_num \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\nlabu\\\\Downloads\\\\train'"
     ]
    }
   ],
   "source": [
    "patient_notes_data = get_note_data()\n",
    "patient_notes_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_for_label_combined_notes(df, label, num_epochs, save=False, save_name=None):\n",
    "    '''\n",
    "    Makes a fine-tuned ClinicalBERT model for a given label with data.\n",
    "\n",
    "    Arguments:\n",
    "    df - DataFrame containing a 'notes' column with the corresponding clinical notes\n",
    "    label - Str that is the name of the column to use as y/labels for the model training\n",
    "    save - Flag to save the model\n",
    "    save_name - What the directory for the model should be named\n",
    "\n",
    "    Returns:\n",
    "    TFAutoModelForSequenceClassification\n",
    "    '''\n",
    "    if save and save_name is None:\n",
    "        save_name = f'{label}_model'\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", from_pt=True)\n",
    "    clinical_notes = list(df['notes'])\n",
    "\n",
    "    # Tokenize and pad the clinical notes\n",
    "    tokenized_notes = tokenizer(clinical_notes, padding='max_length', max_length=512, truncation=True, return_tensors=\"tf\")\n",
    "    tokenized_data = dict(tokenized_notes)\n",
    "    labels = np.array(df[label])\n",
    "\n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3)\n",
    "    model.compile(optimizer=Adam(3e-5), metrics='accuracy')  # No loss argument!\n",
    "\n",
    "    model.fit(tokenized_data, labels, epochs=num_epochs, verbose=True, callbacks=[lr_scheduler])\n",
    "    if save:\n",
    "        tokenizer.save_pretrained(save_name)\n",
    "        model.save_pretrained(save_name)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_models_for_labels(labels, make_model_function, df, epochs, save=False):\n",
    "    '''\n",
    "    Makes a model for each label using the given function, data, and labels.\n",
    "\n",
    "    Arguments:\n",
    "    labels - List[Str] where each string is a label\n",
    "    make_model_function - Callable to make the model that takes in a df and label\n",
    "    df - DataFrame of the data\n",
    "    save - Flag to save each model\n",
    "\n",
    "    Returns:\n",
    "    Dict Str -> TFAutoModelForSequenceClassification\n",
    "    '''\n",
    "    models = {}\n",
    "    for label in labels:\n",
    "        print(f\"Making model for label {label}...\")\n",
    "        model = make_model_function(df, label, epochs, save=save)\n",
    "        print(\"Finished making model.\")\n",
    "        models[label] = model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of make_models_for_labels\n",
    "# # models = make_models_for_labels([\"ABDOMINAL\",\n",
    "#         \"ADVANCED-CAD\",\n",
    "#         \"ALCOHOL-ABUSE\",\n",
    "#         \"ASP-FOR-MI\",\n",
    "#         \"CREATININE\"], make_model_for_label_combined_notes, get_note_data(), epochs=2, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(note):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    pattern = r\"[\\n\\.]\"\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(pattern, note.lower())\n",
    "    # Split the text into tokens\n",
    "    tokens = [nltk.word_tokenize(sentence) for sentence in sentences if sentence.strip()]\n",
    "    # Remove stop words\n",
    "    #filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ABDOMINAL_model were not used when initializing TFBertForSequenceClassification: ['dropout_721']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ABDOMINAL_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at ADVANCED-CAD_model were not used when initializing TFBertForSequenceClassification: ['dropout_759']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ADVANCED-CAD_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at ALCOHOL-ABUSE_model were not used when initializing TFBertForSequenceClassification: ['dropout_797']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ALCOHOL-ABUSE_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at ASP-FOR-MI_model were not used when initializing TFBertForSequenceClassification: ['dropout_835']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ASP-FOR-MI_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at CREATININE_model were not used when initializing TFBertForSequenceClassification: ['dropout_873']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at CREATININE_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at DIETSUPP-2MOS_model were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at DIETSUPP-2MOS_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at DRUG-ABUSE_model were not used when initializing TFBertForSequenceClassification: ['dropout_113']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at DRUG-ABUSE_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at ENGLISH_model were not used when initializing TFBertForSequenceClassification: ['dropout_151']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ENGLISH_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at HBA1C_model were not used when initializing TFBertForSequenceClassification: ['dropout_189']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at HBA1C_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at KETO-1YR_model were not used when initializing TFBertForSequenceClassification: ['dropout_227']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at KETO-1YR_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at MAJOR-DIABETES_model were not used when initializing TFBertForSequenceClassification: ['dropout_265']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at MAJOR-DIABETES_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at MAKES-DECISIONS_model were not used when initializing TFBertForSequenceClassification: ['dropout_303']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at MAKES-DECISIONS_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at MI-6MOS_model were not used when initializing TFBertForSequenceClassification: ['dropout_341']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at MI-6MOS_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABDOMINAL': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x5873e0700>, 'ADVANCED-CAD': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x358bd0a00>, 'ALCOHOL-ABUSE': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x595d43b20>, 'ASP-FOR-MI': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x587342c20>, 'CREATININE': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x32b4bffa0>, 'DIETSUPP-2MOS': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x593ec10c0>, 'DRUG-ABUSE': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x32b0fdd80>, 'ENGLISH': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x32b5fa980>, 'HBA1C': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x6f33cb5e0>, 'KETO-1YR': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x587207f70>, 'MAJOR-DIABETES': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x360e00f10>, 'MAKES-DECISIONS': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x406339b40>, 'MI-6MOS': <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x5dee3ac80>}\n"
     ]
    }
   ],
   "source": [
    "loaded_models = {}\n",
    "for label in labels:\n",
    "    loaded_models[label] = (TFAutoModelForSequenceClassification.from_pretrained(f'{label}_model'), AutoTokenizer.from_pretrained(f'{label}_model'))\n",
    "print(loaded_models)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy_for_label(label, model, tokenizer, verbose=True):\n",
    "    tokenized_notes = tokenizer(patient_notes_data, padding='max_length', max_length=512, truncation=True, return_tensors=\"tf\")\n",
    "    tokenized_data = dict(tokenized_notes)\n",
    "    true_labels = np.array(patient_notes_data[label])\n",
    "\n",
    "    predictions = model.predict(tokenized_data)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    if verbose:\n",
    "        print(f\"{label} Accuracy: {accuracy}\")\n",
    "        \n",
    "    return accuracy\n",
    "\n",
    "label_to_accuracy = {}\n",
    "for label, model_tokenizer in loaded_models.items():\n",
    "    label_to_accuracy[label] = get_accuracy_for_label(label, model_tokenizer[0], model_tokenizer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
