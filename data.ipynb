{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 18)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_df = pd.DataFrame(columns=[\"note1\", \"note2\", \"note3\", \"note4\", \"note5\", \"ABDOMINAL\",\n",
    "\"ADVANCED-CAD\",\n",
    "\"ALCOHOL-ABUSE\",\n",
    "\"ASP-FOR-MI\",\n",
    "\"CREATININE\",\n",
    "\"DIETSUPP-2MOS\",\n",
    "\"DRUG-ABUSE\",\n",
    "\"ENGLISH\",\n",
    "\"HBA1C\",\n",
    "\"KETO-1YR\",\n",
    "\"MAJOR-DIABETES\",\n",
    "\"MAKES-DECISIONS\",\n",
    "\"MI-6MOS\"])\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "folder_name = 'train'\n",
    "directory = os.path.join(current_directory, folder_name)\n",
    "for filename in os.listdir(directory):\n",
    "    if os.path.isfile(os.path.join(directory, filename)):\n",
    "        patient_num = os.path.splitext(filename)[0]\n",
    "        row_to_add = {}\n",
    "        # Load the XML file\n",
    "        tree = ET.parse(os.path.join(directory, filename))\n",
    "        root = tree.getroot()\n",
    "        # Access elements and attributes\n",
    "        for child in root:\n",
    "            if child.tag == \"TEXT\":\n",
    "                notes = child.text.split(\"****************************************************************************************************\")\n",
    "                notes = [note.strip() for note in notes if note.strip()]\n",
    "                i = 1\n",
    "                for note in notes:\n",
    "                    row_to_add[f\"note{i}\"] = note\n",
    "                    i += 1\n",
    "                for j in range(i, 6):\n",
    "                    row_to_add[f\"note{j}\"] = \"\"\n",
    "            if child.tag == \"TAGS\":\n",
    "                for subchild in child:\n",
    "                    row_to_add[subchild.tag] = 1 if subchild.attrib.get('met') == 'met' else 0\n",
    "        overall_df.loc[patient_num] = row_to_add\n",
    "\n",
    "overall_df.head(5)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "https://tfhub.dev/google/BioBERT_Large_TuneD_PubMed/2 does not appear to be a valid module.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\resolver.py:192\u001b[0m, in \u001b[0;36mDownloadManager.download_and_uncompress\u001b[1;34m(self, fileobj, dst_path)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m   \u001b[43mfile_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_tarfile_to_destination\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m   total_size_str \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39mbytes_to_readable_str(\n\u001b[0;32m    195\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_bytes_downloaded, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\file_utils.py:47\u001b[0m, in \u001b[0;36mextract_tarfile_to_destination\u001b[1;34m(fileobj, dst_path, log_function)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract a tarfile. Optional: log the progress.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr|*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfileobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m tgz:\n\u001b[0;32m     48\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m tarinfo \u001b[38;5;129;01min\u001b[39;00m tgz:\n",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\tarfile.py:1836\u001b[0m, in \u001b[0;36mTarFile.open\u001b[1;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[0;32m   1835\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1836\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(name, filemode, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1837\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\tarfile.py:1714\u001b[0m, in \u001b[0;36mTarFile.__init__\u001b[1;34m(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel, copybufsize)\u001b[0m\n\u001b[0;32m   1713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirstmember \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1714\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirstmember \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;66;03m# Move to the end of the archive,\u001b[39;00m\n\u001b[0;32m   1718\u001b[0m     \u001b[38;5;66;03m# before the first empty block.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\tarfile.py:2614\u001b[0m, in \u001b[0;36mTarFile.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2613\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2614\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2615\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EmptyHeaderError:\n",
      "\u001b[1;31mReadError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the BioBERT model from TensorFlow Hub\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m biobert_module \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://tfhub.dev/google/BioBERT_Large_TuneD_PubMed/2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Tokenize clinical note data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m clinical_note \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe patient underwent abdominal surgery for appendicitis last week. Recovery has been uneventful.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\module_v2.py:100\u001b[0m, in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     99\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a string, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m handle)\n\u001b[1;32m--> 100\u001b[0m module_path \u001b[38;5;241m=\u001b[39m \u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m is_hub_module_v1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(_get_module_proto_path(module_path))\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_hub_module_v1:\n",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\module_v2.py:55\u001b[0m, in \u001b[0;36mresolve\u001b[1;34m(handle)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(handle):\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Resolves a module handle into a path.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m  This function works both for plain TF2 SavedModels and the legacy TF1 Hub\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    A string representing the Module path.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\registry.py:49\u001b[0m, in \u001b[0;36mMultiImplRegister.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[0;32m     48\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     50\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:81\u001b[0m, in \u001b[0;36mHttpCompressedFileResolver.__call__\u001b[1;34m(self, handle)\u001b[0m\n\u001b[0;32m     77\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_urlopen(request)\n\u001b[0;32m     78\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m resolver\u001b[38;5;241m.\u001b[39mDownloadManager(handle)\u001b[38;5;241m.\u001b[39mdownload_and_uncompress(\n\u001b[0;32m     79\u001b[0m       response, tmp_dir)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matomic_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lock_file_timeout_sec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\resolver.py:421\u001b[0m, in \u001b[0;36matomic_download\u001b[1;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[0;32m    419\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading TF-Hub Module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, handle)\n\u001b[0;32m    420\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mMakeDirs(tmp_dir)\n\u001b[1;32m--> 421\u001b[0m \u001b[43mdownload_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# Write module descriptor to capture information about which module was\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# downloaded by whom and when. The file stored at the same level as a\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# directory in order to keep the content of the 'model_dir' exactly as it\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# module caching protocol and no code in the TF-Hub library reads its\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# content.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m _write_module_descriptor_file(handle, module_dir)\n",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:78\u001b[0m, in \u001b[0;36mHttpCompressedFileResolver.__call__.<locals>.download\u001b[1;34m(handle, tmp_dir)\u001b[0m\n\u001b[0;32m     75\u001b[0m request \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_compressed_format_query(handle))\n\u001b[0;32m     77\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_urlopen(request)\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDownloadManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_uncompress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nlabu\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\resolver.py:200\u001b[0m, in \u001b[0;36mDownloadManager.download_and_uncompress\u001b[1;34m(self, fileobj, dst_path)\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_download_progress_msg(\n\u001b[0;32m    197\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, Total size: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url, total_size_str),\n\u001b[0;32m    198\u001b[0m       flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mReadError:\n\u001b[1;32m--> 200\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not appear to be a valid module.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url)\n",
      "\u001b[1;31mOSError\u001b[0m: https://tfhub.dev/google/BioBERT_Large_TuneD_PubMed/2 does not appear to be a valid module."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForTokenClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load BioBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/biobert_v1.1_pubmed\")\n",
    "\n",
    "# Load BioBERT model for named entity recognition (NER)\n",
    "model = TFBertForTokenClassification.from_pretrained(\"monologg/biobert_v1.1_pubmed\", num_labels=2)\n",
    "\n",
    "# Clinical note data\n",
    "clinical_note = \"The patient underwent abdominal surgery for appendicitis last week. Recovery has been uneventful.\"\n",
    "\n",
    "# Tokenize clinical note\n",
    "tokenized_input = tokenizer(clinical_note, return_tensors=\"tf\")\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(tokenized_input)\n",
    "\n",
    "# Retrieve predicted labels\n",
    "predicted_labels = tf.argmax(outputs.logits, axis=-1)[0].numpy()\n",
    "\n",
    "# Define the mapping of label indices to entity types\n",
    "label_map = {0: \"O\", 1: \"B-SURG\", 2: \"I-SURG\"}  # Assuming BIO tagging scheme for surgery entities\n",
    "\n",
    "# Extract surgery entities\n",
    "surgery_entities = []\n",
    "current_entity = \"\"\n",
    "for token, label_idx in zip(tokenizer.tokenize(clinical_note), predicted_labels):\n",
    "    label = label_map[label_idx]\n",
    "    if label == \"B-SURG\":\n",
    "        current_entity = token\n",
    "    elif label == \"I-SURG\":\n",
    "        current_entity += \" \" + token\n",
    "    elif label == \"O\" and current_entity:\n",
    "        surgery_entities.append(current_entity)\n",
    "        current_entity = \"\"\n",
    "\n",
    "# Check if abdominal surgery is mentioned\n",
    "abdominal_surgery_mentioned = any(\"abdominal surgery\" in entity.lower() for entity in surgery_entities)\n",
    "\n",
    "# Print the result\n",
    "if abdominal_surgery_mentioned:\n",
    "    print(\"The clinical note mentions that the patient underwent abdominal surgery.\")\n",
    "else:\n",
    "    print(\"No mention of abdominal surgery found in the clinical note.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
